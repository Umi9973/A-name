{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc417e86-4196-4bba-ba65-889f0f1b8ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating [LLAMA2] Prompt SENIOR_P1, Iteration 1\n",
      "Generating [MISTRAL] Prompt SENIOR_P1, Iteration 1\n",
      "Generating [LLAMA2] Prompt SENIOR_P1, Iteration 2\n",
      "Generating [MISTRAL] Prompt SENIOR_P1, Iteration 2\n",
      "Generating [LLAMA2] Prompt SENIOR_P1, Iteration 3\n",
      "Generating [MISTRAL] Prompt SENIOR_P1, Iteration 3\n",
      "Generating [LLAMA2] Prompt SENIOR_P2, Iteration 1\n",
      "Generating [MISTRAL] Prompt SENIOR_P2, Iteration 1\n",
      "Generating [LLAMA2] Prompt SENIOR_P2, Iteration 2\n",
      "Generating [MISTRAL] Prompt SENIOR_P2, Iteration 2\n",
      "Generating [LLAMA2] Prompt SENIOR_P2, Iteration 3\n",
      "Generating [MISTRAL] Prompt SENIOR_P2, Iteration 3\n",
      "Generating [LLAMA2] Prompt SENIOR_P3, Iteration 1\n",
      "Generating [MISTRAL] Prompt SENIOR_P3, Iteration 1\n",
      "Generating [LLAMA2] Prompt SENIOR_P3, Iteration 2\n",
      "Generating [MISTRAL] Prompt SENIOR_P3, Iteration 2\n",
      "Generating [LLAMA2] Prompt SENIOR_P3, Iteration 3\n",
      "Generating [MISTRAL] Prompt SENIOR_P3, Iteration 3\n",
      "Generating [LLAMA2] Prompt SENIOR_P4, Iteration 1\n",
      "Generating [MISTRAL] Prompt SENIOR_P4, Iteration 1\n",
      "Generating [LLAMA2] Prompt SENIOR_P4, Iteration 2\n",
      "Generating [MISTRAL] Prompt SENIOR_P4, Iteration 2\n",
      "Generating [LLAMA2] Prompt SENIOR_P4, Iteration 3\n",
      "Generating [MISTRAL] Prompt SENIOR_P4, Iteration 3\n",
      "Generating [LLAMA2] Prompt SENIOR_P5, Iteration 1\n",
      "Generating [MISTRAL] Prompt SENIOR_P5, Iteration 1\n",
      "Generating [LLAMA2] Prompt SENIOR_P5, Iteration 2\n",
      "Generating [MISTRAL] Prompt SENIOR_P5, Iteration 2\n",
      "Generating [LLAMA2] Prompt SENIOR_P5, Iteration 3\n",
      "Generating [MISTRAL] Prompt SENIOR_P5, Iteration 3\n",
      "✅ Done. Saved to outputs/responses_seniors_llama2_vs_mistral.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from ollama import Client as OllamaClient\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "NUM_GENERATIONS = 3\n",
    "PROMPT_FILE = \"consent_seniors_prompts.json\"  \n",
    "OUTPUT_DIR = Path(\"outputs\")\n",
    "\n",
    "# === UPDATED MODELS (LLAMA2&Mistral) ===\n",
    "MODEL_A = \"ollama_llama2\"\n",
    "MODEL_B = \"ollama_mistral\"\n",
    "\n",
    "LLAMA_MODEL_NAME = \"llama2:latest\"\n",
    "MISTRAL_MODEL_NAME = \"mistral:latest\"\n",
    "\n",
    "# === SETUP ===\n",
    "ollama_client = OllamaClient()\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "with open(PROMPT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    prompts = json.load(f)\n",
    "\n",
    "\n",
    "def generate_with_llama(prompt_text: str) -> str:\n",
    "    \"\"\"Generate with Llama2 local model.\"\"\"\n",
    "    response = ollama_client.chat(\n",
    "        model=LLAMA_MODEL_NAME,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a helpful assistant specialized in explaining digital consent \"\n",
    "                    \"and privacy to older adults (65+). Use clear, plain language.\"\n",
    "                ),\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt_text},\n",
    "        ],\n",
    "    )\n",
    "    return response[\"message\"][\"content\"].strip()\n",
    "\n",
    "\n",
    "def generate_with_mistral(prompt_text: str) -> str:\n",
    "    \"\"\"Generate with Mistral local model.\"\"\"\n",
    "    response = ollama_client.chat(\n",
    "        model=MISTRAL_MODEL_NAME,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a helpful assistant specialized in explaining digital consent \"\n",
    "                    \"and privacy to older adults (65+). Use clear, plain language.\"\n",
    "                ),\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt_text},\n",
    "        ],\n",
    "    )\n",
    "    return response[\"message\"][\"content\"].strip()\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    prompt_id = prompt[\"id\"]\n",
    "    prompt_text = prompt[\"prompt_text\"]\n",
    "    category = prompt.get(\"category\", \"\")\n",
    "    references = prompt.get(\"references\", [])\n",
    "    source_type = prompt.get(\"source_type\", \"\")\n",
    "\n",
    "    for i in range(NUM_GENERATIONS):\n",
    "        gen_idx = i + 1\n",
    "\n",
    "        # === LLAMA2 ===\n",
    "        print(f\"Generating [LLAMA2] Prompt {prompt_id}, Iteration {gen_idx}\")\n",
    "        resp_a = generate_with_llama(prompt_text)\n",
    "        results.append(\n",
    "            {\n",
    "                \"prompt_id\": prompt_id,\n",
    "                \"category\": category,\n",
    "                \"references\": references,\n",
    "                \"source_type\": source_type,\n",
    "                \"model\": MODEL_A,\n",
    "                \"gen_id\": f\"{prompt_id}_llama2_gen{gen_idx}\",\n",
    "                \"response\": resp_a,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # === MISTRAL ===\n",
    "        print(f\"Generating [MISTRAL] Prompt {prompt_id}, Iteration {gen_idx}\")\n",
    "        resp_b = generate_with_mistral(prompt_text)\n",
    "        results.append(\n",
    "            {\n",
    "                \"prompt_id\": prompt_id,\n",
    "                \"category\": category,\n",
    "                \"references\": references,\n",
    "                \"source_type\": source_type,\n",
    "                \"model\": MODEL_B,\n",
    "                \"gen_id\": f\"{prompt_id}_mistral_gen{gen_idx}\",\n",
    "                \"response\": resp_b,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Save JSONL\n",
    "outfile = OUTPUT_DIR / \"responses_seniors_llama2_vs_mistral.jsonl\"\n",
    "with open(outfile, \"w\", encoding=\"utf-8\") as f:\n",
    "    for r in results:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"✅ Done. Saved to {outfile}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dd0656-0412-4b0f-863b-653dfc958777",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
