{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e5c333-3b5c-4211-91b1-755e769572b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring SENIOR_P1_llama2_gen1 ... ✓ OK\n",
      "Scoring SENIOR_P1_mistral_gen1 ... ✓ OK\n",
      "Scoring SENIOR_P1_llama2_gen2 ... ✓ OK\n",
      "Scoring SENIOR_P1_mistral_gen2 ... ✓ OK\n",
      "Scoring SENIOR_P1_llama2_gen3 ... ✓ OK\n",
      "Scoring SENIOR_P1_mistral_gen3 ... ✓ OK\n",
      "Scoring SENIOR_P2_llama2_gen1 ... ✓ OK\n",
      "Scoring SENIOR_P2_mistral_gen1 ... ✓ OK\n",
      "Scoring SENIOR_P2_llama2_gen2 ... ✓ OK\n",
      "Scoring SENIOR_P2_mistral_gen2 ... ✓ OK\n",
      "Scoring SENIOR_P2_llama2_gen3 ... ✓ OK\n",
      "Scoring SENIOR_P2_mistral_gen3 ... ✓ OK\n",
      "Scoring SENIOR_P3_llama2_gen1 ... ✓ OK\n",
      "Scoring SENIOR_P3_mistral_gen1 ... ✓ OK\n",
      "Scoring SENIOR_P3_llama2_gen2 ... ✓ OK\n",
      "Scoring SENIOR_P3_mistral_gen2 ... ✓ OK\n",
      "Scoring SENIOR_P3_llama2_gen3 ... ✓ OK\n",
      "Scoring SENIOR_P3_mistral_gen3 ... ✓ OK\n",
      "Scoring SENIOR_P4_llama2_gen1 ... ✓ OK\n",
      "Scoring SENIOR_P4_mistral_gen1 ... ✓ OK\n",
      "Scoring SENIOR_P4_llama2_gen2 ... ✓ OK\n",
      "Scoring SENIOR_P4_mistral_gen2 ... ✓ OK\n",
      "Scoring SENIOR_P4_llama2_gen3 ... ✓ OK\n",
      "Scoring SENIOR_P4_mistral_gen3 ... ✓ OK\n",
      "Scoring SENIOR_P5_llama2_gen1 ... ✓ OK\n",
      "Scoring SENIOR_P5_mistral_gen1 ... ✓ OK\n",
      "Scoring SENIOR_P5_llama2_gen2 ... ✓ OK\n",
      "Scoring SENIOR_P5_mistral_gen2 ... ✓ OK\n",
      "Scoring SENIOR_P5_llama2_gen3 ... ✓ OK\n",
      "Scoring SENIOR_P5_mistral_gen3 ... ✓ OK\n",
      "\n",
      " Done. Saved scored results to: scored_seniors_llama2_vs_mistral.jsonl\n",
      " Errors (if any) saved to: scoring_errors_seniors_llama2_vs_mistral.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# === API SETUP ===\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\", \"your-api-key-here\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# === UPDATED FILES ===\n",
    "INPUT_FILE = \"responses_seniors_llama2_vs_mistral.jsonl\"\n",
    "OUTPUT_FILE = \"scored_seniors_llama2_vs_mistral.jsonl\"\n",
    "ERROR_FILE = \"scoring_errors_seniors_llama2_vs_mistral.jsonl\"\n",
    "\n",
    "# === SCORING RUBRIC ===\n",
    "rubric = \"\"\"\n",
    "You are an impartial evaluator grading chatbot responses for how well they support senior (65+) users \n",
    "in understanding digital consent. You must score each response along EXACTLY these five dimensions:\n",
    "\n",
    "1. Plain-Language Clarity (0–5)\n",
    "   - Are words simple?\n",
    "   - Are sentences short?\n",
    "   - Is technical jargon avoided?\n",
    "   - Reading ease ~6th–7th grade level.\n",
    "\n",
    "2. Visual Accessibility (0–5)\n",
    "   - Does the response acknowledge or encourage large buttons, high contrast, spacing, or easy-to-see structure?\n",
    "\n",
    "3. Interaction Simplicity (0–5)\n",
    "   - Does the response simplify steps?\n",
    "   - Does it reduce cognitive load?\n",
    "   - Are instructions linear and easy to follow?\n",
    "\n",
    "4. Confirm-Step Safety (0–5)\n",
    "   - Does the response mention multi-step confirmation?\n",
    "   - Does it prevent accidental consent?\n",
    "   - Does it offer “go back / change your choice” assurances?\n",
    "\n",
    "5. Empathy & Senior-Friendly Tone (0–5)\n",
    "   - Warm, patient, reassuring tone.\n",
    "   - Avoids anxiety, pressure, or rushed wording.\n",
    "   - Addresses potential memory or comprehension challenges.\n",
    "\n",
    "Your output MUST be valid JSON only. No commentary.\n",
    "\n",
    "You MUST output:\n",
    "{\n",
    "  \"plain_language\": X,\n",
    "  \"visual_accessibility\": X,\n",
    "  \"interaction_simplicity\": X,\n",
    "  \"confirm_steps\": X,\n",
    "  \"empathy_tone\": X,\n",
    "  \"weighted_score\": X\n",
    "}\n",
    "\n",
    "Use these weights to compute weighted_score:\n",
    "- Plain language: 0.30\n",
    "- Visual accessibility: 0.20\n",
    "- Interaction simplicity: 0.20\n",
    "- Confirm steps: 0.20\n",
    "- Empathy tone: 0.10\n",
    "\"\"\"\n",
    "\n",
    "# === FIXED & ROBUST score_response() ===\n",
    "def score_response(prompt, response):\n",
    "    user_prompt = f\"\"\"\n",
    "Evaluate the following response for senior-friendly consent UX.\n",
    "\n",
    "Prompt:\n",
    "{prompt}\n",
    "\n",
    "Response:\n",
    "{response}\n",
    "\n",
    "Return ONLY a JSON object exactly in this structure:\n",
    "{{\n",
    "  \"plain_language\": 0-5,\n",
    "  \"visual_accessibility\": 0-5,\n",
    "  \"interaction_simplicity\": 0-5,\n",
    "  \"confirm_steps\": 0-5,\n",
    "  \"empathy_tone\": 0-5,\n",
    "  \"weighted_score\": final_score_between_0_and_5\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=0.0,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": rubric},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        reply = completion.choices[0].message.content.strip()\n",
    "\n",
    "        # === ROBUST JSON PARSING START ===\n",
    "        import re\n",
    "\n",
    "        # Try direct JSON load\n",
    "        try:\n",
    "            return json.loads(reply)\n",
    "        except json.JSONDecodeError:\n",
    "            # Attempt to extract JSON object with regex\n",
    "            match = re.search(r\"\\{[\\s\\S]*\\}\", reply)\n",
    "            if match:\n",
    "                try:\n",
    "                    return json.loads(match.group(0))\n",
    "                except Exception as e2:\n",
    "                    raise RuntimeError(\n",
    "                        f\"GPT-4o scoring failed: JSON extraction unsuccessful. Raw reply: {reply}\"\n",
    "                    ) from e2\n",
    "            else:\n",
    "                raise RuntimeError(\n",
    "                    f\"GPT-4o scoring failed: no JSON object detected in reply. Raw reply: {reply}\"\n",
    "                )\n",
    "        # === ROBUST JSON PARSING END ===\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"GPT-4o scoring failed: {e}\")\n",
    "\n",
    "\n",
    "# === PROCESS THE FILE ===\n",
    "with open(INPUT_FILE, \"r\") as fin, \\\n",
    "     open(OUTPUT_FILE, \"w\") as fout, \\\n",
    "     open(ERROR_FILE, \"w\") as ferr:\n",
    "\n",
    "    for line in fin:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            item = json.loads(line)\n",
    "            prompt_id = item.get(\"prompt_id\")\n",
    "            gen_id = item.get(\"gen_id\")\n",
    "            prompt_text = item.get(\"prompt_text\", \"\")\n",
    "            response_text = item.get(\"response\", \"\")\n",
    "\n",
    "            print(f\"Scoring {gen_id} ...\", end=\" \")\n",
    "\n",
    "            if not response_text:\n",
    "                raise ValueError(\"Empty model response.\")\n",
    "\n",
    "            score_data = score_response(prompt_text, response_text)\n",
    "            item[\"score\"] = score_data\n",
    "\n",
    "            fout.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "            print(\"✓ OK\")\n",
    "\n",
    "        except Exception as e:\n",
    "            ferr.write(json.dumps({\n",
    "                \"prompt_id\": item.get(\"prompt_id\"),\n",
    "                \"gen_id\": item.get(\"gen_id\"),\n",
    "                \"error\": str(e)\n",
    "            }) + \"\\n\")\n",
    "            print(f\"⚠ Error: {e}\")\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "print(f\"\\n Done. Saved scored results to: {OUTPUT_FILE}\")\n",
    "print(f\" Errors (if any) saved to: {ERROR_FILE}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ac075e-dcb6-4f1a-a52a-9b3fb62b9af9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
