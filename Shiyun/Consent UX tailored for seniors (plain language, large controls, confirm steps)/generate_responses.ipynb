{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc417e86-4196-4bba-ba65-889f0f1b8ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating [GPT-4o] Prompt SENIOR_P1, Iteration 1\n",
      "Generating [Ollama] Prompt SENIOR_P1, Iteration 1\n",
      "Generating [GPT-4o] Prompt SENIOR_P1, Iteration 2\n",
      "Generating [Ollama] Prompt SENIOR_P1, Iteration 2\n",
      "Generating [GPT-4o] Prompt SENIOR_P1, Iteration 3\n",
      "Generating [Ollama] Prompt SENIOR_P1, Iteration 3\n",
      "Generating [GPT-4o] Prompt SENIOR_P2, Iteration 1\n",
      "Generating [Ollama] Prompt SENIOR_P2, Iteration 1\n",
      "Generating [GPT-4o] Prompt SENIOR_P2, Iteration 2\n",
      "Generating [Ollama] Prompt SENIOR_P2, Iteration 2\n",
      "Generating [GPT-4o] Prompt SENIOR_P2, Iteration 3\n",
      "Generating [Ollama] Prompt SENIOR_P2, Iteration 3\n",
      "Generating [GPT-4o] Prompt SENIOR_P3, Iteration 1\n",
      "Generating [Ollama] Prompt SENIOR_P3, Iteration 1\n",
      "Generating [GPT-4o] Prompt SENIOR_P3, Iteration 2\n",
      "Generating [Ollama] Prompt SENIOR_P3, Iteration 2\n",
      "Generating [GPT-4o] Prompt SENIOR_P3, Iteration 3\n",
      "Generating [Ollama] Prompt SENIOR_P3, Iteration 3\n",
      "Generating [GPT-4o] Prompt SENIOR_P4, Iteration 1\n",
      "Generating [Ollama] Prompt SENIOR_P4, Iteration 1\n",
      "Generating [GPT-4o] Prompt SENIOR_P4, Iteration 2\n",
      "Generating [Ollama] Prompt SENIOR_P4, Iteration 2\n",
      "Generating [GPT-4o] Prompt SENIOR_P4, Iteration 3\n",
      "Generating [Ollama] Prompt SENIOR_P4, Iteration 3\n",
      "Generating [GPT-4o] Prompt SENIOR_P5, Iteration 1\n",
      "Generating [Ollama] Prompt SENIOR_P5, Iteration 1\n",
      "Generating [GPT-4o] Prompt SENIOR_P5, Iteration 2\n",
      "Generating [Ollama] Prompt SENIOR_P5, Iteration 2\n",
      "Generating [GPT-4o] Prompt SENIOR_P5, Iteration 3\n",
      "Generating [Ollama] Prompt SENIOR_P5, Iteration 3\n",
      "✅ Done. Saved to outputs/responses_seniors_gpt4o_vs_ollama.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-keys-here\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "from ollama import Client as OllamaClient\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "NUM_GENERATIONS = 3\n",
    "PROMPT_FILE = \"consent_seniors_prompts.json\"  \n",
    "OUTPUT_DIR = Path(\"outputs\")\n",
    "MODEL_A = \"gpt-4o\"\n",
    "MODEL_B = \"ollama_mistral\"        \n",
    "OLLAMA_MODEL_NAME = \"mistral\"    \n",
    "\n",
    "# === SETUP ===\n",
    "openai_client = OpenAI()\n",
    "ollama_client = OllamaClient()\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "with open(PROMPT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    prompts = json.load(f)\n",
    "\n",
    "\n",
    "def generate_with_gpt4o(prompt_text: str) -> str:\n",
    "    \"\"\"Call GPT-4o to generate a response.\"\"\"\n",
    "    chat = openai_client.chat.completions.create(\n",
    "        model=MODEL_A,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a helpful assistant specialized in explaining digital consent \"\n",
    "                    \"and privacy to older adults (65+). Use clear, plain language.\"\n",
    "                ),\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt_text},\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=512,\n",
    "    )\n",
    "    return chat.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def generate_with_ollama(prompt_text: str) -> str:\n",
    "    \"\"\"Call Ollama local model to generate a response.\"\"\"\n",
    "    response = ollama_client.chat(\n",
    "        model=OLLAMA_MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": (\n",
    "                 \"You are a helpful assistant specialized in explaining digital consent \"\n",
    "                 \"and privacy to older adults (65+). Use clear, plain language.\"\n",
    "             )},\n",
    "            {\"role\": \"user\", \"content\": prompt_text},\n",
    "        ],\n",
    "    )\n",
    "    # ollama's python client returns dict with 'message'\n",
    "    return response[\"message\"][\"content\"].strip()\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    prompt_id = prompt[\"id\"]\n",
    "    prompt_text = prompt[\"prompt_text\"]\n",
    "    category = prompt.get(\"category\", \"\")\n",
    "    references = prompt.get(\"references\", [])\n",
    "    source_type = prompt.get(\"source_type\", \"\")\n",
    "\n",
    "    for i in range(NUM_GENERATIONS):\n",
    "        gen_idx = i + 1\n",
    "\n",
    "        print(f\"Generating [GPT-4o] Prompt {prompt_id}, Iteration {gen_idx}\")\n",
    "        response_a = generate_with_gpt4o(prompt_text)\n",
    "        results.append(\n",
    "            {\n",
    "                \"prompt_id\": prompt_id,\n",
    "                \"category\": category,\n",
    "                \"references\": references,\n",
    "                \"source_type\": source_type,\n",
    "                \"model\": MODEL_A,\n",
    "                \"gen_id\": f\"{prompt_id}_gpt4o_gen{gen_idx}\",\n",
    "                \"response\": response_a,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(f\"Generating [Ollama] Prompt {prompt_id}, Iteration {gen_idx}\")\n",
    "        response_b = generate_with_ollama(prompt_text)\n",
    "        results.append(\n",
    "            {\n",
    "                \"prompt_id\": prompt_id,\n",
    "                \"category\": category,\n",
    "                \"references\": references,\n",
    "                \"source_type\": source_type,\n",
    "                \"model\": MODEL_B,\n",
    "                \"gen_id\": f\"{prompt_id}_ollama_gen{gen_idx}\",\n",
    "                \"response\": response_b,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# save to jsonl\n",
    "outfile = OUTPUT_DIR / \"responses_seniors_gpt4o_vs_ollama.jsonl\"\n",
    "with open(outfile, \"w\", encoding=\"utf-8\") as f:\n",
    "    for r in results:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"✅ Done. Saved to {outfile}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dd0656-0412-4b0f-863b-653dfc958777",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
