
# Consent UX Clarity & Accessibility Demo

This demo evaluates two chat-based foundation models (GPT-4o and Ollama) on their ability to clearly explain consent-related topics. The evaluation is based on five ethical communication dimensions and follows a reproducible pipeline.

## ğŸ“ Project Structure

```
your_project/
â”œâ”€â”€ generate_responses.py           # Generates model responses using GPT-4o and Ollama
â”œâ”€â”€ score_responses.py              # Scores responses using GPT-4o based on ethical rubric
â”œâ”€â”€ responses_gpt4o_vs_ollama.jsonl # Collected responses from both models
â”œâ”€â”€ scored_responses.jsonl          # Scored results with clarity, tone, completeness, etc.
â”œâ”€â”€ scoring_errors.jsonl            # Log of failed or invalid response scorings (if any)
â”œâ”€â”€ README.md                       # This file
```

## ğŸ§ª Usage Workflow

### Step 1: Generate Responses
Run the script to generate responses for multiple prompts from GPT-4o and Ollama:
```bash
python generate_responses.py
```

- Requires OpenAI API Key in environment.
- Ollama must be running locally with a model like `mistral` pulled.

### Step 2: Score Responses
Score all generated responses with GPT-4o using the provided ethical rubric:
```bash
python score_responses.py
```

- Ensure OpenAI API key is valid and set in the script or environment.
- Results are saved to `scored_responses.jsonl`.

### Step 3: Analyze Results
You can inspect or load `scored_responses.jsonl` into a pandas DataFrame for further analysis, visualization, or exporting.

### Optional: Error Handling
If any response failed to be scored (e.g., empty or invalid JSON), they will be logged to:
```
scoring_errors.jsonl
```

## ğŸ“‹ Scoring Dimensions

Each response is rated 0â€“5 for:

- **Clarity**
- **Accessibility**
- **Tone Neutrality/Helpfulness**
- **Completeness**
- **Opt-Out Support**

The final score is the average of the five.

---

For questions or improvements, feel free to edit this README or the pipeline scripts.
