{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4d9d8ec-5db5-45da-a4da-d420be75206f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-2.8.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting ollama\n",
      "  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.13/site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.13/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.13/site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.10.0 (from openai)\n",
      "  Downloading jiter-0.12.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.13/site-packages (from openai) (2.10.3)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.13/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.13/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.13/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
      "Downloading openai-2.8.1-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.12.0-cp313-cp313-macosx_11_0_arm64.whl (318 kB)\n",
      "Downloading ollama-0.6.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: jiter, openai, ollama\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/3\u001b[0m [ollama]2m1/3\u001b[0m [openai]\n",
      "\u001b[1A\u001b[2KSuccessfully installed jiter-0.12.0 ollama-0.6.1 openai-2.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install openai ollama\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2a117a3-020b-42c0-9910-eb30ff445d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No, I'm not a physical entity and therefore don't have the capability to run. I am here to assist you by providing information and answering questions. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "#Ensure that ollama is working\n",
    "from ollama import Client\n",
    "\n",
    "client = Client()\n",
    "response = client.chat(model=\"mistral\", messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Hello, are you running?\"}\n",
    "])\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b0cd80b-15b0-4ecb-a311-57759902fb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating [GPT-4o] Prompt P1, Iteration 1\n",
      "Generating [Ollama] Prompt P1, Iteration 1\n",
      "Generating [GPT-4o] Prompt P1, Iteration 2\n",
      "Generating [Ollama] Prompt P1, Iteration 2\n",
      "Generating [GPT-4o] Prompt P1, Iteration 3\n",
      "Generating [Ollama] Prompt P1, Iteration 3\n",
      "Generating [GPT-4o] Prompt P2, Iteration 1\n",
      "Generating [Ollama] Prompt P2, Iteration 1\n",
      "Generating [GPT-4o] Prompt P2, Iteration 2\n",
      "Generating [Ollama] Prompt P2, Iteration 2\n",
      "Generating [GPT-4o] Prompt P2, Iteration 3\n",
      "Generating [Ollama] Prompt P2, Iteration 3\n",
      "Generating [GPT-4o] Prompt P3, Iteration 1\n",
      "Generating [Ollama] Prompt P3, Iteration 1\n",
      "Generating [GPT-4o] Prompt P3, Iteration 2\n",
      "Generating [Ollama] Prompt P3, Iteration 2\n",
      "Generating [GPT-4o] Prompt P3, Iteration 3\n",
      "Generating [Ollama] Prompt P3, Iteration 3\n",
      "Generating [GPT-4o] Prompt P4, Iteration 1\n",
      "Generating [Ollama] Prompt P4, Iteration 1\n",
      "Generating [GPT-4o] Prompt P4, Iteration 2\n",
      "Generating [Ollama] Prompt P4, Iteration 2\n",
      "Generating [GPT-4o] Prompt P4, Iteration 3\n",
      "Generating [Ollama] Prompt P4, Iteration 3\n",
      "Generating [GPT-4o] Prompt P5, Iteration 1\n",
      "Generating [Ollama] Prompt P5, Iteration 1\n",
      "Generating [GPT-4o] Prompt P5, Iteration 2\n",
      "Generating [Ollama] Prompt P5, Iteration 2\n",
      "Generating [GPT-4o] Prompt P5, Iteration 3\n",
      "Generating [Ollama] Prompt P5, Iteration 3\n",
      "âœ… Done. Saved to outputs/responses_gpt4o_vs_ollama.jsonl\n"
     ]
    }
   ],
   "source": [
    "#Running the prompts and save the output automatically\n",
    "!python3 generate_responses.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f5194b7-9eec-4e37-8ec4-7f1309c2e5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring: P1_gpt4o_gen1\n",
      "âš ï¸ Error scoring P1_gpt4o_gen1: Expecting value: line 1 column 1 (char 0)\n",
      "Scoring: P1_ollama_gen1\n",
      "âš ï¸ Error scoring P1_ollama_gen1: Expecting value: line 1 column 1 (char 0)\n",
      "Scoring: P1_gpt4o_gen2\n",
      "âš ï¸ Error scoring P1_gpt4o_gen2: Expecting value: line 1 column 1 (char 0)\n",
      "Scoring: P1_ollama_gen2\n",
      "Scoring: P1_gpt4o_gen3\n",
      "Scoring: P1_ollama_gen3\n",
      "âš ï¸ Error scoring P1_ollama_gen3: Expecting value: line 1 column 1 (char 0)\n",
      "Scoring: P2_gpt4o_gen1\n",
      "Scoring: P2_ollama_gen1\n",
      "Scoring: P2_gpt4o_gen2\n",
      "Scoring: P2_ollama_gen2\n",
      "Scoring: P2_gpt4o_gen3\n",
      "Scoring: P2_ollama_gen3\n",
      "Scoring: P3_gpt4o_gen1\n",
      "âš ï¸ Error scoring P3_gpt4o_gen1: Expecting value: line 1 column 1 (char 0)\n",
      "Scoring: P3_ollama_gen1\n",
      "âš ï¸ Error scoring P3_ollama_gen1: Expecting value: line 1 column 1 (char 0)\n",
      "Scoring: P3_gpt4o_gen2\n",
      "Scoring: P3_ollama_gen2\n",
      "âš ï¸ Error scoring P3_ollama_gen2: Expecting value: line 1 column 1 (char 0)\n",
      "Scoring: P3_gpt4o_gen3\n",
      "Scoring: P3_ollama_gen3\n",
      "Scoring: P4_gpt4o_gen1\n",
      "âš ï¸ Error scoring P4_gpt4o_gen1: Expecting value: line 1 column 1 (char 0)\n",
      "Scoring: P4_ollama_gen1\n",
      "âš ï¸ Error scoring P4_ollama_gen1: Expecting value: line 1 column 1 (char 0)\n",
      "Scoring: P4_gpt4o_gen2\n",
      "âš ï¸ Error scoring P4_gpt4o_gen2: Expecting value: line 1 column 1 (char 0)\n",
      "Scoring: P4_ollama_gen2\n",
      "Scoring: P4_gpt4o_gen3\n",
      "Scoring: P4_ollama_gen3\n",
      "Scoring: P5_gpt4o_gen1\n",
      "Scoring: P5_ollama_gen1\n",
      "Scoring: P5_gpt4o_gen2\n",
      "Scoring: P5_ollama_gen2\n",
      "Scoring: P5_gpt4o_gen3\n",
      "âš ï¸ Error scoring P5_gpt4o_gen3: Expecting value: line 1 column 1 (char 0)\n",
      "Scoring: P5_ollama_gen3\n",
      "âœ… Done. Scores saved to scored_responses.jsonl\n"
     ]
    }
   ],
   "source": [
    "!python3 score_responses.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e5a2fa7-c0fe-48a2-b03a-73cd623904f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§¾ å…±å‘ç°é—®é¢˜æ¡ç›®æ•°: 0\n"
     ]
    }
   ],
   "source": [
    "#double check all of the outputs and figure out the error\n",
    "import json\n",
    "\n",
    "input_path = \"/Users/hp/Desktop/ä»£ç é›†/assignment3/your project/outputs/responses_gpt4o_vs_ollama.jsonl\"\n",
    "bad_entries = []\n",
    "\n",
    "with open(input_path, 'r') as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            print(f\"[Line {i}] âš ï¸ ç©ºè¡Œï¼Œè·³è¿‡\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ex = json.loads(line)\n",
    "        except Exception as e:\n",
    "            print(f\"[Line {i}] âŒ JSONè§£æå¤±è´¥ â†’ {e}\")\n",
    "            bad_entries.append((i, 'INVALID_JSON', line[:100]))\n",
    "            continue\n",
    "\n",
    "        resp = ex.get(\"response\", None)\n",
    "        gen_id = ex.get(\"gen_id\", f\"line_{i}\")\n",
    "\n",
    "        # å„ç±»å¼‚å¸¸æƒ…å†µæ£€æŸ¥\n",
    "        if resp is None:\n",
    "            print(f\"[{gen_id}] âš ï¸ response æ˜¯ None\")\n",
    "            bad_entries.append((gen_id, 'None'))\n",
    "\n",
    "        elif isinstance(resp, (list, dict)) and len(resp) == 0:\n",
    "            print(f\"[{gen_id}] âš ï¸ response æ˜¯ç©ºåˆ—è¡¨æˆ–ç©ºå­—å…¸ï¼š{type(resp)}\")\n",
    "            bad_entries.append((gen_id, 'EMPTY_COLLECTION'))\n",
    "\n",
    "        elif isinstance(resp, str):\n",
    "            if resp.strip() == \"\":\n",
    "                print(f\"[{gen_id}] âš ï¸ response æ˜¯ç©ºå­—ç¬¦ä¸²\")\n",
    "                bad_entries.append((gen_id, 'EMPTY_STRING'))\n",
    "\n",
    "        else:\n",
    "            print(f\"[{gen_id}] âš ï¸ response ç±»å‹ä¸æ˜¯å­—ç¬¦ä¸²ï¼š{type(resp)}\")\n",
    "            bad_entries.append((gen_id, f'INVALID_TYPE: {type(resp)}'))\n",
    "\n",
    "print(\"\\nğŸ§¾ å…±å‘ç°é—®é¢˜æ¡ç›®æ•°:\", len(bad_entries))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d546adc-be29-4578-861a-5931072ad075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/Users/hp/Desktop/ä»£ç é›†/assignment3/your project/outputs/score_responses_v2.py\"\u001b[0m, line \u001b[35m60\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    with \u001b[31mopen\u001b[0m\u001b[1;31m(INPUT_FILE, \"r\")\u001b[0m as fin, \\\n",
      "         \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[1;35mFileNotFoundError\u001b[0m: \u001b[35m[Errno 2] No such file or directory: 'responses_gpt4o_vs_ollama.jsonl'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 outputs/score_responses_v2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195d0186-efbf-4f7e-b425-30da0e651287",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
