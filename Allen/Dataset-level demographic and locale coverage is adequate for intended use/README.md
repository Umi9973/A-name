# Dataset Representativeness Evaluation ‚Äì Evidence Collection & Automated ADA Scoring

This repository contains all files required to reproduce the evidence‚Äëcollection and automated scoring workflow for evaluating **‚ÄúDataset‚Äëlevel demographic and locale coverage is adequate for intended use‚Äù**, an **L4 branch under Fairness** in the AI Ethics Index measurement project.

This evaluation follows the **ADA (Automated Document Analysis)** methodology and uses evidence extracted from authoritative dataset‚Äëdocumentation frameworks (Datasheets for Datasets, Model Cards, NIST AI RMF).  
The scoring rubric is fully customized for dataset representativeness and implemented in `score_dataset_coverage.py`.

---

## üìÅ Repository Structure

| File | Description |
|------|-------------|
| `dataset_coverage_prompt.txt` | Reproducible ADA evidence‚Äëcollection prompt for this L4 branch. |
| `score_dataset_coverage.py` | Full pipeline script generating evidence (GPT‚Äë4o & Ollama) and performing strict ADA scoring using GPT‚Äë4o as judge. |
| `gpt4o_dataset_coverage_evidence.txt` | Detailed ADA‚Äëstyle evidence report generated by GPT‚Äë4o. |
| `ollama_dataset_coverage_evidence.txt` | Detailed ADA‚Äëstyle evidence report generated by the local Ollama model. |
| `gpt4o_scoring_weighted.json` | Raw JSON scoring results for GPT‚Äë4o evidence. |
| `ollama_scoring_weighted.json` | Raw JSON scoring results for Ollama evidence. |
| `gpt4o_scoring_weighted.xlsx` | Weighted scoring table for GPT‚Äë4o evidence. |
| `ollama_scoring_weighted.xlsx` | Weighted scoring table for Ollama evidence. |
| `dataset_coverage_scoring_weighted.xlsx` | **Combined scoring table** for both models. |
| `README.md` | Documentation for methodology and scoring workflow. |

---

## üîÑ Workflow Overview

The workflow contains **three major stages**:

---

### **1. Evidence Collection (ADA ‚Äì Automated Document Analysis)**

Evidence is generated automatically based on the structured prompt in `dataset_coverage_prompt.txt`.

Each model generates a **1500‚Äì2500‚Äëword, audit‚Äëready ADA evidence report** including:

#### Extracted representativeness dimensions:
- Demographic coverage  
- Geographic / locale representation  
- Language distribution  
- Sampling methodology  
- Dataset limitations  

#### Required sources:
- Datasheets for Datasets  
- Model Cards for Model Reporting  
- NIST AI RMF  
- Other official dataset documentation  

#### Required structure:
- Overview of sources  
- Evidence table (with verbatim excerpts)  
- Full analysis per representativeness dimension  
- Missing‚Äëdisclosure section  

Model outputs:
- `gpt4o_dataset_coverage_evidence.txt`  
- `ollama_dataset_coverage_evidence.txt`

---

### **2. Automated ADA Scoring (Strict Rubric)**

Scoring is performed by **GPT‚Äë4o** using a strict, six‚Äëdimension ADA rubric:

1. Evidence Extraction Quality  
2. Coverage of Representativeness Dimensions  
3. Structure & Formatting  
4. Relevance & Faithfulness  
5. Identification of Missing Disclosures  
6. Audit Usefulness  

Each dimension is scored **0‚Äì5**, with strict guidelines:
- **5 = Rare / near‚Äëperfect**
- **2‚Äì4 = Typical realistic range**
- **0‚Äì1 = Weak or missing**

Weighted totals use the official L4 scoring weights:
| Criterion | Weight |
|----------|--------|
| Evidence Extraction Quality | **0.25** |
| Coverage of Representativeness Dimensions | **0.25** |
| Structure & Formatting | 0.15 |
| Relevance & Faithfulness | 0.15 |
| Missing Disclosures | 0.10 |
| Audit Usefulness | 0.10 |

Outputs:
- JSON: `gpt4o_scoring_weighted.json`, `ollama_scoring_weighted.json`  
- Excel: `gpt4o_scoring_weighted.xlsx`, `ollama_scoring_weighted.xlsx`

---

### **3. Combined Evaluation Output**

The script merges both models‚Äô results into:

- **`dataset_coverage_scoring_weighted.xlsx`**

This file enables:
- Cross‚Äëmodel comparison  
- Scoring matrix integration  
- Fairness evaluation reporting  

---

## ‚öôÔ∏è Automated Execution

Run the full pipeline with:

```bash
python score_dataset_coverage.py
```

This script will:

1. Load `dataset_coverage_prompt.txt`  
2. Generate detailed evidence (GPT‚Äë4o + Ollama)  
3. Save both `.txt` evidence files  
4. Score both evidence reports using GPT‚Äë4o  
5. Output JSON + Excel scoring files  
6. Produce a combined scoring file  

All outputs are saved to the same directory.

---

## üîÅ Reproducibility

This repository ensures:

- Deterministic evidence‚Äëcollection prompts  
- Strict, fixed scoring rubric  
- Model‚Äëspecific text evidence snapshots  
- Human‚Äëreadable and machine‚Äëreadable results  
- Repeatable scoring pipeline (`score_dataset_coverage.py`)

Ideal for:
- Cross‚Äëmodel comparisons  
- Scoring reliability analysis  
- Academic reproducibility  

---

## üìå Summary

This repository delivers:

- Reproducible ADA‚Äëbased evidence extraction  
- Strict and transparent scoring rubric  
- Automated scoring for multiple models  
- Combined evaluation output for FAIRNESS L4  
- Full audit trail for dataset‚Äërepresentativeness assessment  

It forms a complete evaluation pipeline for the  
**Dataset‚Äëlevel Demographic and Locale Coverage** L4 branch of the AI Ethics Index.
