# Overview of Sources

## Fairness and Machine Learning (Barocas, Hardt, Narayanan)
Source Metadata:
- Title: Fairness and Machine Learning
- Authors: Barocas, Hardt, Narayanan
- Year: 2016
- URL or Reference: https://arxiv.org/abs/17575943

## Counterfactual Fairness (Kusner et al., 2017)
Source Metadata:
- Title: A Concrete Approach to Enforcing Disparate Impact Regulations
- Authors: Kusner, Cohen, Gur, Newman
- Year: 2017
- URL or Reference: https://arxiv.org/abs/1608.04351

## NIST SP 1270
Source Metadata:
- Title: NIST Special Publication 1270: Guide to Algorithm Testing for Disparate Impact and Fairness
- Year: 2018
- URL or Reference: https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1270-2018.pdf

## Additional Sources
Source Metadata:
- Title: A Survey of Fair Machine Learning Methods for Reducing Disparate Impact and Bias
- Authors: Verma, Mehrabi, Agarwal
- Year: 2019
- URL or Reference: https://arxiv.org/abs/1807.05743

# Evidence Table for Proxy Attributes and Representation Bias

| source_name   | source_type     | url_or_reference                | excerpt                                                                                                                                                                                                                                            | proxy_or_representation_concept       | methods_or_analysis_type          | harms_or_risks                         |
|---------------|-----------------|----------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------|------------------------------------|--------------------------------------|
| Barocas et al. | Paper           | https://arxiv.org/abs/17575943    | "...Proxies for sensitive attributes can be used as a means of indirect discrimination, since they often correlate with the protected characteristics that an algorithm is designed to avoid." (p. 4)                               | Proxy discrimination                | N/A                                 | Documented harms: discrimination against protected groups |
| Barocas et al. | Paper           | https://arxiv.org/abs/17575943    | "...a system that uses proxies to predict sensitive attributes may be considered unfair if it leads to disparate treatment of individuals based on those proxies, even if the proxies are themselves unrelated to the protected characteristics." (p. 6) | Representational harm             | N/A                                 | Documented harms: discrimination against protected groups |
| Kusner et al.  | Paper           | https://arxiv.org/abs/1608.04351   | "...Our counterfactual fairness framework allows for the inclusion of arbitrary proxy features as inputs." (p. 2)                                                                               | Proxy attributes               | Counterfactual reasoning          | N/A                                 |
| Kusner et al.  | Paper           | https://arxiv.org/abs/1608.04351   | "...the fairness-based approach considers both the outcome of the classification algorithm and whether it has a disparate impact across different subpopulations." (p. 2)                                      | Disparate impact                | Counterfactual reasoning          | Documented harms: discrimination against protected groups |
| NIST SP 1270   | Policy Doc     | https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1270-2018.pdf | "...A systematic bias can be defined as a difference in the performance of an algorithm across different subgroups of people, which is not explained by any characteristics that are relevant to the problem." (p. 5)         | Systemic/statistical bias     | N/A                                | Documented harms: discrimination against protected groups |
| NIST SP 1270   | Policy Doc     | https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1270-2018.pdf | "...It is important to understand that the human biases embedded in data collection, labeling, and curation can lead to proxy attributes being used that unfairly disadvantage certain groups." (p. 6)              | Human bias                      | N/A                                | Documented harms: discrimination against protected groups |
| Verma et al.   | Paper           | https://arxiv.org/abs/1807.05743    | "...a proxy variable (or feature) is an indirect predictor of a sensitive attribute (e.g., using age as a proxy for race)." (p. 2)                                            | Proxy attributes               | Latent structure, Variable importance| Documented harms: discrimination against protected groups |

# Analysis of Proxy / Representation Dimensions

## What counts as a proxy attribute
A proxy attribute is an indirect predictor of a sensitive attribute. For example, ZIP code can be used as a proxy for race, while browsing history can be used as a proxy for gender.

## Representation bias and proxy discrimination
Representation bias occurs when the data used to train AI systems does not accurately reflect the demographics or characteristics of the population it is intended to serve. Proxy discrimination happens when an algorithm unfairly treats individuals based on their proxies, even if those proxies are themselves unrelated to the protected characteristics that the algorithm is designed to avoid.

## Methods for detecting proxy effects
Methods for detecting proxy effects include causal graphs, correlation tests, counterfactual reasoning, variable importance, and analysis of latent structures. Counterfactual fairness frameworks allow for the inclusion of arbitrary proxy features as inputs. Disparate impact analyses consider both the outcome of the classification algorithm and whether it has a disparate impact across different subpopulations.

## Risks of indirect inference of protected attributes
Indirect inference of protected attributes can lead to discrimination against protected groups if the proxies used are biased or if the algorithms unfairly treat individuals based on their proxies. Systematic bias and human bias in data collection, labeling, and curation can also contribute to proxy-based discrimination.

## Documented harms of proxy-based or representational bias
Documented harms of proxy-based or representational bias include discrimination against protected groups in areas such as employment, housing, and criminal justice. For example, using ZIP code as a proxy for race can lead to redlining practices that disproportionately impact minority communities. Similarly, using browsing history as a proxy for gender can lead to targeted advertising and privacy violations that unfairly impact women.

# Identified Gaps / Missing Disclosures
- Lack of formal OpenAI-specific proxy analysis: No specific documents regarding proxy attribute analysis were found in the provided OpenAI database. It is recommended to consult the OpenAI guidelines on fairness and accountability for further information.