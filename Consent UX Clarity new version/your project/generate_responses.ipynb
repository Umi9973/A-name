{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa9e6fef-d252-4c32-89ea-670da964189e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating [LLAMA] Prompt P1, Iteration 1\n",
      "Generating [MISTRAL] Prompt P1, Iteration 1\n",
      "Generating [LLAMA] Prompt P1, Iteration 2\n",
      "Generating [MISTRAL] Prompt P1, Iteration 2\n",
      "Generating [LLAMA] Prompt P1, Iteration 3\n",
      "Generating [MISTRAL] Prompt P1, Iteration 3\n",
      "Generating [LLAMA] Prompt P2, Iteration 1\n",
      "Generating [MISTRAL] Prompt P2, Iteration 1\n",
      "Generating [LLAMA] Prompt P2, Iteration 2\n",
      "Generating [MISTRAL] Prompt P2, Iteration 2\n",
      "Generating [LLAMA] Prompt P2, Iteration 3\n",
      "Generating [MISTRAL] Prompt P2, Iteration 3\n",
      "Generating [LLAMA] Prompt P3, Iteration 1\n",
      "Generating [MISTRAL] Prompt P3, Iteration 1\n",
      "Generating [LLAMA] Prompt P3, Iteration 2\n",
      "Generating [MISTRAL] Prompt P3, Iteration 2\n",
      "Generating [LLAMA] Prompt P3, Iteration 3\n",
      "Generating [MISTRAL] Prompt P3, Iteration 3\n",
      "Generating [LLAMA] Prompt P4, Iteration 1\n",
      "Generating [MISTRAL] Prompt P4, Iteration 1\n",
      "Generating [LLAMA] Prompt P4, Iteration 2\n",
      "Generating [MISTRAL] Prompt P4, Iteration 2\n",
      "Generating [LLAMA] Prompt P4, Iteration 3\n",
      "Generating [MISTRAL] Prompt P4, Iteration 3\n",
      "Generating [LLAMA] Prompt P5, Iteration 1\n",
      "Generating [MISTRAL] Prompt P5, Iteration 1\n",
      "Generating [LLAMA] Prompt P5, Iteration 2\n",
      "Generating [MISTRAL] Prompt P5, Iteration 2\n",
      "Generating [LLAMA] Prompt P5, Iteration 3\n",
      "Generating [MISTRAL] Prompt P5, Iteration 3\n",
      "✅ Done. Saved to outputs/responses_llama_vs_mistral.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from ollama import Client as OllamaClient\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "NUM_GENERATIONS = 3\n",
    "PROMPT_FILE = \"consent_prompts_en.json\"\n",
    "OUTPUT_DIR = Path(\"outputs\")\n",
    "\n",
    "# === RESEARCH MODELS (replace GPT4o with LLAMA + MISTRAL) ===\n",
    "MODEL_A = \"ollama_llama2\"\n",
    "MODEL_B = \"ollama_mistral\"\n",
    "\n",
    "LLAMA_MODEL_NAME = \"llama2:latest\"\n",
    "MISTRAL_MODEL_NAME = \"mistral:latest\"\n",
    "\n",
    "\n",
    "# === SETUP ===\n",
    "ollama_client = OllamaClient()\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Load prompts\n",
    "with open(PROMPT_FILE, \"r\") as f:\n",
    "    prompts = json.load(f)\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# MODEL GENERATION\n",
    "# --------------------\n",
    "def generate_with_llama(prompt_text):\n",
    "    response = ollama_client.chat(\n",
    "        model=LLAMA_MODEL_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt_text}]\n",
    "    )\n",
    "    return response[\"message\"][\"content\"].strip()\n",
    "\n",
    "\n",
    "def generate_with_mistral(prompt_text):\n",
    "    response = ollama_client.chat(\n",
    "        model=MISTRAL_MODEL_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt_text}]\n",
    "    )\n",
    "    return response[\"message\"][\"content\"].strip()\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# MAIN LOOP\n",
    "# --------------------\n",
    "results = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    prompt_id = prompt[\"id\"]\n",
    "    prompt_text = prompt[\"prompt_text\"]\n",
    "\n",
    "    for i in range(NUM_GENERATIONS):\n",
    "        # === LLAMA ===\n",
    "        print(f\"Generating [LLAMA] Prompt {prompt_id}, Iteration {i+1}\")\n",
    "        resp_a = generate_with_llama(prompt_text)\n",
    "        results.append({\n",
    "            \"prompt_id\": prompt_id,\n",
    "            \"model\": MODEL_A,\n",
    "            \"gen_id\": f\"{prompt_id}_llama_gen{i+1}\",\n",
    "            \"response\": resp_a\n",
    "        })\n",
    "\n",
    "        # === MISTRAL ===\n",
    "        print(f\"Generating [MISTRAL] Prompt {prompt_id}, Iteration {i+1}\")\n",
    "        resp_b = generate_with_mistral(prompt_text)\n",
    "        results.append({\n",
    "            \"prompt_id\": prompt_id,\n",
    "            \"model\": MODEL_B,\n",
    "            \"gen_id\": f\"{prompt_id}_mistral_gen{i+1}\",\n",
    "            \"response\": resp_b\n",
    "        })\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# SAVE OUTPUT\n",
    "# --------------------\n",
    "outfile = OUTPUT_DIR / \"responses_llama_vs_mistral.jsonl\"\n",
    "with open(outfile, \"w\") as f:\n",
    "    for r in results:\n",
    "        f.write(json.dumps(r) + \"\\n\")\n",
    "\n",
    "print(f\"✅ Done. Saved to {outfile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e2a0a3-d499-4553-a8ba-9d9c93df3480",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
